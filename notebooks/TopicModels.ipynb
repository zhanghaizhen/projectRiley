{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "from HTMLParser import HTMLParser\n",
    "import datetime\n",
    "import cPickle as pickle\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Files\n",
    "all_file = '/Users/lekha/galvanize/capstone/projectRiley/data/withindgroup/all_withindgroup.txt'\n",
    "tech_file = '/Users/lekha/galvanize/capstone/projectRiley/data/withindgroup/tech_withind.txt'\n",
    "hadoop_all = '/Users/lekha/galvanize/capstone/projectRiley/data/withindgroup/all_hadoop.txt'\n",
    "hadoop_tech = '/Users/lekha/galvanize/capstone/projectRiley/data/withindgroup/tech_hadoop.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfall1 = pd.read_csv(all_file, sep=\"|\")\n",
    "dftech1 = pd.read_csv(tech_file, sep=\"|\")\n",
    "dfh = pd.read_csv(hadoop_all, sep=\"|\")\n",
    "dfht = pd.read_csv(hadoop_tech, sep=\"|\")\n",
    "\n",
    "dftech_all = pd.concat([dftech1, pd.read_csv(hadoop_tech, sep=\"|\")], axis=0)\n",
    "df_all2 = pd.concat([dfall1, pd.read_csv(hadoop_all, sep=\"|\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_vocab = str(f_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_file = '/Users/lekha/galvanize/capstone/projectRiley/data/withindgroup/all2.txt'\n",
    "tech_file = '/Users/lekha/galvanize/capstone/projectRiley/data/withindgroup/tech_all2.txt'\n",
    "\n",
    "df1 = pd.read_csv(all_file, sep=\"|\")\n",
    "df2 = pd.read_csv(tech_file, sep=\"|\")\n",
    "df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def missing(df):\n",
    "    if df.summary == 'missing' or df.num_tokens == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "    \n",
    "def lenx(mystr):\n",
    "    return len(mystr.split())\n",
    "\n",
    "\n",
    "def avgchrs(mytokens):\n",
    "    tw = len(mytokens)    \n",
    "    num_chars = 0\n",
    "    for word in mytokens:\n",
    "        num_chars += len(word)        \n",
    "    return num_chars/tw\n",
    "\n",
    "\n",
    "def remove_digits(mystr):\n",
    "    '''\n",
    "    INPUT: list of tokens \n",
    "    OUTPUT: list of tokens with digits removed\n",
    "    '''\n",
    "    return [word for word in mystr if not word.isdigit()]\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    row = remove_digits(tokens)\n",
    "    stems = stem_tokens(row, stemmer)\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_no_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    row = remove_digits(tokens)\n",
    "    stems = stem_tokens(row, stemmer)\n",
    "    return tokens \n",
    "\n",
    "def preprocess_df(df):\n",
    "    # Feature Engineering before running the prediction code\n",
    "    df['class'] = np.ones(len(df))\n",
    "    df['class'] = df['gender_forced'].apply(lambda x: 0 if x == 'female' else 1)\n",
    "\n",
    "    df['summ_tokens'] = df['summary'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "    df['num_tokens'] = df['summ_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "    # Add feature for missing summary\n",
    "    df['summ_missing'] = df.apply(missing, axis = 1)\n",
    "\n",
    "    # Only include rows with summaries\n",
    "    df = df[df['summ_missing'] == 0]\n",
    "\n",
    "    # Some Nan rows refuse to go without this\n",
    "    df = df[pd.notnull(df['summary'])]\n",
    "\n",
    "    print (\"Length of DF after removing rows with missing Summaries:\\n\")\n",
    "    print (len(df))\n",
    "\n",
    "    df['avg_len'] = df['summ_tokens'].apply(lambda x: avgchrs(x))\n",
    "\n",
    "    # lexical diversity = number of unique tokens / total number of tokens\n",
    "    df['lex_diversity'] = df['summ_tokens'].apply(lambda x: len(set(x))/len(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the top 20 words and weights for each topic\n",
    "def print_top_weights(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        top_weights = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        #print (top_weights)\n",
    "        print (\" \".join([\"{0}, {1}\".format(feature_names[x], topic[x]) for x in top_weights]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "# Print the most probable topic for each document/profile\n",
    "def profiles_by_topic(W):\n",
    "    top_idx = np.zeros([W.shape[0],1], dtype=int)\n",
    "    for row_idx, row in enumerate(W):\n",
    "        topic_idx = row.argsort()[-1]  \n",
    "        top_idx[row_idx] = topic_idx\n",
    "    topics, counts = np.unique(top_idx, return_counts=True)\n",
    "    print (np.asarray((topics, counts)).T)\n",
    "    return top_idx\n",
    "\n",
    "           \n",
    "def run_topic_model_tfidf(X, stopwords):\n",
    "    print (\"Bag of Words, Tfidf\\n\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(analyzer = 'word', tokenizer = tokenize_no_stem, stop_words = stopwords, max_features = 5000)\n",
    "    word_frequencies = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an \n",
    "    # array\n",
    "    word_frequencies = word_frequencies.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # NMF Model to determine topics\n",
    "    nmf_model = NMF(n_components=7, init='random', random_state=0)\n",
    "    W = nmf_model.fit_transform(word_frequencies)\n",
    "    # H: Topics * Words\n",
    "    H = nmf_model.components_\n",
    "    print (H.shape, W.shape)\n",
    "    \n",
    "    n_top_words = 20\n",
    "    # Print weights and topics for the top 20 topics\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        top_weights = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        print (\" \".join([\"{0}\".format(feature_names[x]) for x in top_weights]))\n",
    "    print()\n",
    "    \n",
    "    # Highest Weighted Topic for each profile\n",
    "    print (\"Number of profiles by topic\\n\")\n",
    "    top_idx = profiles_by_topic(W) \n",
    "    \n",
    "    return nmf_model, W, top_idx\n",
    "\n",
    "\n",
    "#tokenize: function that is stemming using SnowballStemmer\n",
    "#stopwords: custom stop words\n",
    "def run_topic_model_countv(X, stopwords):\n",
    "    print (\"Bag of words, Count Vectorizer...\\n\")\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = tokenize_no_stem,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = stopwords,   \\\n",
    "                                 max_df = 0.7, \\\n",
    "                                 min_df = 5, \\\n",
    "                                 max_features = 5000) \n",
    "\n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of \n",
    "    # strings.\n",
    "    word_frequencies = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an \n",
    "    # array\n",
    "    word_frequencies = word_frequencies.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # NMF Model to determine topics\n",
    "    nmf_model = NMF(n_components=4, init='random', random_state=0)\n",
    "    W = nmf_model.fit_transform(word_frequencies)\n",
    "    # H: Topics * Words\n",
    "    H = nmf_model.components_\n",
    "    print (H.shape, W.shape)\n",
    "    \n",
    "    n_top_words = 20\n",
    "    # Print weights and topics for the top 20 topics\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        top_weights = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        #print (\" \".join([\"{0}\".format(feature_names[x]) for x in top_weights]))\n",
    "        print (\" \".join([\"{0}, {1}\".format(feature_names[x], topic[x]) for x in top_weights]))\n",
    "    print()\n",
    "    \n",
    "    print (\"Number of profiles by Topic\\n\")\n",
    "    top_idx = profiles_by_topic(W)\n",
    "    \n",
    "    #print (\"Example Profiles by Topic\\n\")\n",
    "    \n",
    "    return nmf_model, W, top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_models_by_df(df):\n",
    "    df = preprocess_df(df)\n",
    "    females = df[df['gender'] == 'female']\n",
    "    males = df[df['gender'] == 'male']\n",
    "\n",
    "    X = df['summary']\n",
    "    y = np.array(df['class'])\n",
    "    print (\"Topics for ALL profiles: TFIDF\\n\")\n",
    "    nmf_model, W, top_idx = run_topic_model_tfidf(X, stopwords)\n",
    "    \n",
    "    print (\"Topics for ALL profiles: Count Vectorizer\\n\")\n",
    "    nmf_model, W, top_idx = run_topic_model_countv(X, stopwords)\n",
    "\n",
    "    X = females['summary']\n",
    "    y = np.array(females['class'])\n",
    "    print (\"Topics for FEMALE profiles: TFIDF\\n\")\n",
    "    nmf_model, W, top_idx = run_topic_model_tfidf(X, stopwords)\n",
    "    print (\"Topics for FEMALE profiles: Count Vectorizer\\n\")\n",
    "    nmf_model, W, top_idx = run_topic_model_countv(X, stopwords)\n",
    "\n",
    "    X = males['summary']\n",
    "    y = np.array(males['class'])\n",
    "    print (\"Topics for MALE profiles: TFIDF\\n\")\n",
    "    nmf_model, W, top_idx = run_topic_model_tfidf(X, stopwords)\n",
    "    print (\"Topics for MALE profiles: Count Vectorizer\\n\")\n",
    "    nmf_model, W, top_idx = run_topic_model_countv(X, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DF after removing rows with missing Summaries:\n",
      "\n",
      "26850\n",
      "Length of DF after removing rows with missing Summaries:\n",
      "\n",
      "26850\n",
      "Topics for ALL profiles: TFIDF\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'run_topic_model_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3683ec790fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpreprocess_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtopic_models_by_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7602dd516689>\u001b[0m in \u001b[0;36mtopic_models_by_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Topics for ALL profiles: TFIDF\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnmf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_topic_model_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Topics for ALL profiles: Count Vectorizer\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'run_topic_model_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "df= preprocess_df(df)\n",
    "topic_models_by_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
